{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Prediction - Using NLP to classify Political Leanings of Tweets\n",
    "\n",
    "Inspired by Assignment \\#5, we can train a Support Vector Classifier to classify the political leanings of Russian bot Tweets based on their content. This SVC will be trained specifically to \"guess\" if a Tweet is left-leaning or right-leaning. It is trained based on the account_type its posting account was classified as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first install dependencies and import modules used to train our NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/jovyan/.local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning Data\n",
    "\n",
    "Load our Tweets into a CSV, and then clean it:\n",
    "\n",
    "1. We only want to focus on English Tweets in the USA region.\n",
    "2. All content should be lower-cased, since the sample data uses capitalizations somewhat randomly\n",
    "3. Finally, because we are only interested in Left/Right tweets, filter out any Tweets which accounts are not \"Left\" or \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('./data/IRAhandle_tweets_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "english_tweets = tweets_df[(tweets_df['language']=='English') & (tweets_df['region']==\"United States\")]\n",
    "english_tweets['content'] = english_tweets['content'].str.lower() # screw the warnings!\n",
    "english_tweets = english_tweets[(english_tweets['account_type'] == 'Right') | (english_tweets['account_type'] == 'Left')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Right', 'Left'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['account_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick way for us to make sure to some light degree our data is properly cleaned\n",
    "assert(sorted(set(english_tweets['account_type'])) == [\"Left\", \"Right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Matrix Representations\n",
    "\n",
    "In our matrix, we need to assign values to our groups.\n",
    "\n",
    "We assign 0.0 for \"Right\" and 1.0 for \"Left\" account_types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"Right\": return 0.0\n",
    "    elif label == \"Left\": return 1.0\n",
    "    else: return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then now, we use this function and apply it to our english_tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tweets['y'] = english_tweets['account_type'].apply(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, make sure we're doing things right - this is useful for quick validations\n",
    "assert(sorted(set(english_tweets['y'])) == [0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Our Data\n",
    "\n",
    "We will vectorize our data using the TF-IDF method. Our SGDClassifier will be trained using these vectors. \n",
    "\n",
    "A hardware limitation we encountered was that our hardware devices aren't really capable of transforming more than about 20,000 tweets at any given time. Thus, we will batch-train the SGDClassifier by using 2000 TF-IDF fit-transformed data which uses random samples of size 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our SVC that we will batch-train\n",
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                             tokenizer=word_tokenize, \n",
    "                             stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the SGDClassifier\n",
    "\n",
    "### Fitting the Data\n",
    "Batch-train the SGDClassifier with `partial_fit()` by passing in 2,000 random samples from our Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    # Train using sample sizes of 20,000\n",
    "    data = english_tweets.sample(20000)\n",
    "    et_x = vectorizer.fit_transform(data['content']).toarray()\n",
    "    et_y = data['y']\n",
    "    # Once we have this sample data transformed and fitted into the vectorizer, train CLF with it\n",
    "    # Note: vectorizer.fit_transform() will change [vocab] shape each time cause fit_transform(), this is okay\n",
    "    clf.partial_fit(et_x, et_y, classes=np.unique(et_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Accurate Are We?\n",
    "\n",
    "Let's fetch a random sample of 20,000 tweets (doesn't have to be 20,000), and run an analysis on how accurate the SGDClassifier is:\n",
    "\n",
    "TODO: This isn't really accurate right now because we've only done 1 iteration on a small set, but once it's integrated into the master code it'll be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 52752)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how accurate (or inaccurate this is)\n",
    "\n",
    "test_data = english_tweets.sample(20000)\n",
    "test_data_vector = vectorizer.transform(test_data['content']).toarray()\n",
    "test_data_y = test_data['y']\n",
    "\n",
    "results = clf.predict(test_data_vector)\n",
    "print(classification_report(test_data_y, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
